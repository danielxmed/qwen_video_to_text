accelerate==1.12.0
av==16.0.1
boto3==1.42.17
botocore==1.42.17
certifi==2025.11.12
charset-normalizer==3.4.4
decord==0.6.0
einops==0.8.1
ffmpeg==1.4
filelock==3.20.1
flash_attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.3/flash_attn-2.8.3+cu12torch2.8cxx11abiTRUE-cp311-cp311-linux_x86_64.whl#sha256=3d41b2fc55753faa7f45d6568ea73a96b96afb48b82994ab9b49bcbcb6c87588
fsspec==2025.12.0
hf-xet==1.2.0
huggingface-hub==0.36.0
idna==3.11
Jinja2==3.1.6
jmespath==1.0.1
MarkupSafe==3.0.3
mpmath==1.3.0
networkx==3.6.1
numpy==2.4.0
nvidia-cublas-cu12==12.8.4.1
nvidia-cuda-cupti-cu12==12.8.90
nvidia-cuda-nvrtc-cu12==12.8.93
nvidia-cuda-runtime-cu12==12.8.90
nvidia-cudnn-cu12==9.10.2.21
nvidia-cufft-cu12==11.3.3.83
nvidia-cufile-cu12==1.13.1.3
nvidia-curand-cu12==10.3.9.90
nvidia-cusolver-cu12==11.7.3.90
nvidia-cusparse-cu12==12.5.8.93
nvidia-cusparselt-cu12==0.7.1
nvidia-nccl-cu12==2.27.3
nvidia-nvjitlink-cu12==12.8.93
nvidia-nvshmem-cu12==3.3.20
nvidia-nvtx-cu12==12.8.90
packaging==25.0
pillow==12.0.0
protobuf==6.33.2
psutil==7.2.0
python-dateutil==2.9.0.post0
python-dotenv==1.2.1
PyYAML==6.0.3
regex==2025.11.3
requests==2.32.5
s3transfer==0.16.0
safetensors==0.7.0
six==1.17.0
sympy==1.14.0
tokenizers==0.22.1
torch==2.8.0+cu128
torchaudio==2.8.0+cu128
torchcodec==0.7.0
torchvision==0.23.0+cu128
tqdm==4.67.1
transformers==4.57.3
triton==3.4.0
typing_extensions==4.15.0
urllib3==2.6.2
